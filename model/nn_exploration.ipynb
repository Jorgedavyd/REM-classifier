{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "This notebook is for tuning hyperparameters and finding the most accurate architecture for this aplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data importation\n",
    "\n",
    "dataframe = pd.read_csv('data/data.csv')\n",
    "\n",
    "#Getting the columns\n",
    "columns = dataframe.columns.values\n",
    "\n",
    "#Cleaning the dataframe\n",
    "df = dataframe.drop(columns = columns[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "#Defining input features and target feature\n",
    "\n",
    "input_cols = columns[2:-1]\n",
    "targets = columns[-1]\n",
    "\n",
    "#Getting the tensors\n",
    "inputs, targets = dataframe_to_torch(df,input_cols, targets)\n",
    "\n",
    "#Creating the dataset\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "\n",
    "#Cross-validation set:20% training set: 80%\n",
    "\n",
    "val_size = round(0.2*len(dataset))\n",
    "\n",
    "train_ds, val_ds = random_split(dataset , [len(dataset) - val_size, val_size])\n",
    "\n",
    "batch_size = 128  # Change based in dataset size and  GPU capacity (also known as a hyperparameter)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and ouput values of the hidden layers\n",
    "architecture = (256,128,64,32)\n",
    "\n",
    "# Model definition\n",
    "model = DeepNeuralNetwork(4, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNeuralNetwork(\n",
      "  (overall_structure): Sequential(\n",
      "    (layer_1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer_2): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer_3): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer_4): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00005, train_loss: 0.5815, val_loss: 0.5278, val_acc: 0.7633\n",
      "Epoch [1], last_lr: 0.00008, train_loss: 0.5130, val_loss: 0.4964, val_acc: 0.7762\n",
      "Epoch [2], last_lr: 0.00013, train_loss: 0.4767, val_loss: 0.4641, val_acc: 0.7820\n",
      "Epoch [3], last_lr: 0.00020, train_loss: 0.4500, val_loss: 0.4413, val_acc: 0.7941\n",
      "Epoch [4], last_lr: 0.00028, train_loss: 0.4304, val_loss: 0.4298, val_acc: 0.7936\n",
      "Epoch [5], last_lr: 0.00037, train_loss: 0.4199, val_loss: 0.4202, val_acc: 0.7943\n",
      "Epoch [6], last_lr: 0.00047, train_loss: 0.4155, val_loss: 0.4170, val_acc: 0.7924\n",
      "Epoch [7], last_lr: 0.00057, train_loss: 0.4136, val_loss: 0.4160, val_acc: 0.7939\n",
      "Epoch [8], last_lr: 0.00067, train_loss: 0.4124, val_loss: 0.4222, val_acc: 0.7959\n",
      "Epoch [9], last_lr: 0.00076, train_loss: 0.4123, val_loss: 0.4181, val_acc: 0.7941\n",
      "Epoch [10], last_lr: 0.00084, train_loss: 0.4110, val_loss: 0.4152, val_acc: 0.7951\n",
      "Epoch [11], last_lr: 0.00091, train_loss: 0.4120, val_loss: 0.4123, val_acc: 0.7934\n",
      "Epoch [12], last_lr: 0.00096, train_loss: 0.4109, val_loss: 0.4133, val_acc: 0.7959\n",
      "Epoch [13], last_lr: 0.00099, train_loss: 0.4092, val_loss: 0.4167, val_acc: 0.7917\n",
      "Epoch [14], last_lr: 0.00100, train_loss: 0.4089, val_loss: 0.4125, val_acc: 0.7953\n",
      "Epoch [15], last_lr: 0.00100, train_loss: 0.4084, val_loss: 0.4103, val_acc: 0.7947\n",
      "Epoch [16], last_lr: 0.00099, train_loss: 0.4088, val_loss: 0.4101, val_acc: 0.7957\n",
      "Epoch [17], last_lr: 0.00098, train_loss: 0.4063, val_loss: 0.4101, val_acc: 0.7994\n",
      "Epoch [18], last_lr: 0.00097, train_loss: 0.4064, val_loss: 0.4088, val_acc: 0.7969\n",
      "Epoch [19], last_lr: 0.00095, train_loss: 0.4067, val_loss: 0.4098, val_acc: 0.7996\n",
      "Epoch [20], last_lr: 0.00093, train_loss: 0.4055, val_loss: 0.4068, val_acc: 0.7963\n",
      "Epoch [21], last_lr: 0.00090, train_loss: 0.4064, val_loss: 0.4061, val_acc: 0.7956\n",
      "Epoch [22], last_lr: 0.00088, train_loss: 0.4044, val_loss: 0.4072, val_acc: 0.7970\n",
      "Epoch [23], last_lr: 0.00085, train_loss: 0.4048, val_loss: 0.4068, val_acc: 0.7972\n",
      "Epoch [24], last_lr: 0.00081, train_loss: 0.4037, val_loss: 0.4067, val_acc: 0.7974\n",
      "Epoch [25], last_lr: 0.00078, train_loss: 0.4041, val_loss: 0.4054, val_acc: 0.7970\n",
      "Epoch [26], last_lr: 0.00074, train_loss: 0.4005, val_loss: 0.4067, val_acc: 0.7971\n",
      "Epoch [27], last_lr: 0.00070, train_loss: 0.4008, val_loss: 0.4044, val_acc: 0.7986\n",
      "Epoch [28], last_lr: 0.00065, train_loss: 0.4022, val_loss: 0.4031, val_acc: 0.7990\n",
      "Epoch [29], last_lr: 0.00061, train_loss: 0.3997, val_loss: 0.4090, val_acc: 0.7994\n",
      "Epoch [30], last_lr: 0.00057, train_loss: 0.3999, val_loss: 0.4014, val_acc: 0.7958\n",
      "Epoch [31], last_lr: 0.00052, train_loss: 0.3983, val_loss: 0.4033, val_acc: 0.8010\n",
      "Epoch [32], last_lr: 0.00048, train_loss: 0.3968, val_loss: 0.4020, val_acc: 0.8004\n",
      "Epoch [33], last_lr: 0.00043, train_loss: 0.3978, val_loss: 0.4015, val_acc: 0.8000\n",
      "Epoch [34], last_lr: 0.00039, train_loss: 0.3965, val_loss: 0.3982, val_acc: 0.8018\n",
      "Epoch [35], last_lr: 0.00035, train_loss: 0.3969, val_loss: 0.3995, val_acc: 0.8049\n",
      "Epoch [36], last_lr: 0.00030, train_loss: 0.3968, val_loss: 0.3985, val_acc: 0.8025\n",
      "Epoch [37], last_lr: 0.00026, train_loss: 0.3944, val_loss: 0.4016, val_acc: 0.8033\n",
      "Epoch [38], last_lr: 0.00022, train_loss: 0.3929, val_loss: 0.3984, val_acc: 0.8022\n",
      "Epoch [39], last_lr: 0.00019, train_loss: 0.3935, val_loss: 0.3998, val_acc: 0.8001\n",
      "Epoch [40], last_lr: 0.00015, train_loss: 0.3913, val_loss: 0.3973, val_acc: 0.8061\n",
      "Epoch [41], last_lr: 0.00012, train_loss: 0.3916, val_loss: 0.3979, val_acc: 0.8047\n",
      "Epoch [42], last_lr: 0.00010, train_loss: 0.3889, val_loss: 0.3962, val_acc: 0.8037\n",
      "Epoch [43], last_lr: 0.00007, train_loss: 0.3898, val_loss: 0.3986, val_acc: 0.8032\n",
      "Epoch [44], last_lr: 0.00005, train_loss: 0.3900, val_loss: 0.3933, val_acc: 0.8061\n",
      "Epoch [45], last_lr: 0.00003, train_loss: 0.3897, val_loss: 0.3964, val_acc: 0.8043\n",
      "Epoch [46], last_lr: 0.00002, train_loss: 0.3880, val_loss: 0.4037, val_acc: 0.8023\n",
      "Epoch [47], last_lr: 0.00001, train_loss: 0.3870, val_loss: 0.3967, val_acc: 0.8049\n",
      "Epoch [48], last_lr: 0.00000, train_loss: 0.3866, val_loss: 0.4027, val_acc: 0.8035\n",
      "Epoch [49], last_lr: 0.00000, train_loss: 0.3891, val_loss: 0.3952, val_acc: 0.8057\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "epochs = 50\n",
    "max_lr = 0.0001\n",
    "grad_clip = 0.007\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam\n",
    "\n",
    "#Training\n",
    "history = []\n",
    "\n",
    "history += fit(epochs, max_lr, model, train_loader, val_loader, weight_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
